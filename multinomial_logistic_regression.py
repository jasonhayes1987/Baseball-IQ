# -*- coding: utf-8 -*-
"""Multinomial Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yQpkCoo72iHKHTenj8gsPX-ldvaw2rkw
"""

!python --version

!nvcc --version

!git clone https://github.com/rapidsai/rapidsai-csp-utils.git

!python rapidsai-csp-utils/colab/pip-install.py

# install pybaseball for data
! pip install pybaseball

# import pybaseball
# from pybaseball import schedule_and_record
import pandas as pd
import cupy as cp
import numpy as np
# import cudf
import glob
from pathlib import Path
from google.colab import drive
drive.mount('/content/drive')

# team batting advanced stats per year
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Batting/per season/advanced*.csv')
team_adv_bat_y = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
team_adv_bat_y['Season'] = pd.to_datetime(team_adv_bat_y['Season'], format='%Y').dt.year
team_adv_bat_y

# team batting batted ball stats per year
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Batting/per season/batted_balls*.csv')
team_bat_bat_y = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
team_bat_bat_y['Season'] = pd.to_datetime(team_bat_bat_y['Season'], format='%Y').dt.year
team_bat_bat_y

# Team batting advanced stats per game
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Batting/per game/advanced*.csv')
team_adv_bat_g = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
team_adv_bat_g['Date'] = pd.to_datetime(team_adv_bat_g['Date'])
team_adv_bat_g

# Team offensive batted ball stats per game
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Batting/per game/batted_balls*.csv')
team_bat_bat_g = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
team_bat_bat_g['Date'] = pd.to_datetime(team_bat_bat_g['Date'])
team_bat_bat_g

# starting pitcher advanced stats per year
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Pitching/Starting Pitching/per season/advanced*.csv')
sp_adv_y = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
sp_adv_y['Season'] = pd.to_datetime(sp_adv_y['Season'], format='%Y').dt.year
sp_adv_y

# starting pitcher batted ball stats per year
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Pitching/Starting Pitching/per season/batted_balls*.csv')
sp_bat_y = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
sp_bat_y['Season'] = pd.to_datetime(sp_bat_y['Season'], format='%Y').dt.year
sp_bat_y

# starting pitcher advanced stats per game
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Pitching/Starting Pitching/per game/advanced*.csv')
sp_adv_g = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
sp_adv_g['Date'] = pd.to_datetime(sp_adv_g['Date'])
sp_adv_g

# starting pitcher batted ball stats per game
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Pitching/Starting Pitching/per game/batted_balls*.csv')
sp_bat_g = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
sp_bat_g['Date'] = pd.to_datetime(sp_bat_g['Date'])
sp_bat_g

# relief pitcher advanced stats per year
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Pitching/Relief Pitching/per season/advanced*.csv')
rp_adv_y = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
rp_adv_y['Season'] = pd.to_datetime(rp_adv_y['Season'], format='%Y').dt.year
rp_adv_y

# relief pitcher batted ball stats per year
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Pitching/Relief Pitching/per season/batted_balls*.csv')
rp_bat_y = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
rp_bat_y['Season'] = pd.to_datetime(rp_bat_y['Season'], format='%Y').dt.year
rp_bat_y

# relief pitcher advanced stats per game
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Pitching/Relief Pitching/per game/advanced*.csv')
rp_adv_g = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
rp_adv_g['Date'] = pd.to_datetime(rp_adv_g['Date'])
rp_adv_g

# relief pitcher batted ball stats per game
files = glob.glob('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/Pitching/Relief Pitching/per game/batted_balls*.csv')
rp_bat_g = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)
# convert dates to timestamps
rp_bat_g['Date'] = pd.to_datetime(rp_bat_g['Date'])
rp_bat_g

# team defensive stats per year
team_def_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_def_data.csv')
# rename Season column to get rid of ghost character in column name
# team_def_data = team_def_data.rename(columns={"﻿Season":"Season"})
# convert dates to timestamps
team_def_data['Season'] = pd.to_datetime(team_def_data['Season'], format='%Y').dt.year
# convert team column data to object type so it is supported by cudf apply func.
# team_def_data['Team'] = team_def_data['Team'].astype('object')
team_def_data

team_abbr = {'Mariners':'SEA', 'Royals':'KCR', 'Diamondbacks':'ARI',
                 'Rays':'TBR', 'Rangers':'TEX', 'Phillies':'PHI',
                 'Devil Rays':'TBD', 'Twins':'MIN', 'Reds':'CIN',
                 'Athletics':'OAK', 'Astros':'HOU', 'Giants':'SFG',
                 'Red Sox':'BOS', 'Braves':'ATL', 'Dodgers':'LAD',
                 'Padres':'SDP', 'Cubs':'CHC', 'Tigers':'DET',
                 'Cardinals':'STL', 'Orioles':'BAL', 'Angels':['ANA','LAA'],
                 'Yankees':'NYY', 'Indians':'CLE', 'Nationals':'WSN',
                 'Blue Jays':'TOR', 'Brewers':'MIL', 'White Sox':'CHW',
                 'Mets':'NYM', 'Marlins':['FLA','MIA'], 'Pirates':'PIT',
                 'Guardians':'CLE', 'Cleveland':'CLE', 'Rockies':'COL',
                 'Expos':'MON',
                 }

def get_team_abbreviation(row):
    team_name = row['Team']
    year = row['Season']
    if isinstance(team_abbr[team_name], str):
        return team_abbr[team_name]
    elif team_name == 'Angels' and year < 2005:
        return team_abbr[team_name][0]
    elif team_name == 'Marlins' and year < 2012:
        return team_abbr[team_name][0]
    else:
        return team_abbr[team_name][1]

# add a new column to the dataframe with the team abbreviations
team_def_data['team_abbr'] = team_def_data.apply(get_team_abbreviation, axis=1)
# drop Team column and rename team_abbr to Team
team_def_data = team_def_data.drop(columns='Team').rename(columns={'team_abbr':'Team'})
team_def_data = team_def_data.drop(columns=['rTS','FRM','OAA','RAA'])
team_def_data

# check for NaN values in the database
team_def_data.isna().sum()

# save team_def_data as team_def_data_formatted.csv
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_def_data_formatted.csv')
team_def_data.to_csv(filepath, index=False)

# load team_def_data_formatted
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_def_data_formatted.csv')
team_def_data_f = pd.read_csv(filepath)

team_def_data_f

# get every game from 2002 to 2022 and pitchers used to extract starting pitchers 
games = pd.DataFrame()
for year in range(2002,2023):
    print(f'year {year}')
    for team in sp_adv_y['Team'].unique():
        print(f'---team {team}')
        if team == 'WSN' and year < 2005:
            continue
        elif team == 'MON' and year > 2004:
            continue
        elif team == 'TBR' and year < 2008:
            continue
        elif team == 'TBD' and year > 2007:
            continue
        elif team == 'LAA' and year < 2005:
            continue
        elif team == 'ANA' and year > 2004:
            continue
        elif team == 'MIA' and year < 2012:
            continue
        elif team == 'FLA' and year > 2011:
            continue
        else:
            db = pybaseball.team_game_logs(year, team, 'pitching')
            db['Team'] = team
            db['Year'] = year
            games = pd.concat([games,db])

filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/raw_games.csv')
games.to_csv(filepath, index=False)

raw_games = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/raw_games.csv')

raw_games

# create a copy of the raw_games database called games which will be used to clean up the data
games = raw_games.copy()

games

# remove game num from date
def remove_game_num(date):
    tags = ['(1)','(2)','susp']
    for tag in tags:
        date = date.replace(tag,'')
    return date

def home_away_score(row):
    scores = row['Rslt'].split(',')[1].split('-')
    if row['Home'] == True:
        row['Home Score'] = scores[0]
        row['Away Score'] = scores[1]
    else:
        row['Home Score'] = scores[1]
        row['Away Score'] = scores[0]

    return row

# create starting pitcher columnn for each game in input
def starting_pitcher(row):
    row['Starting Pitcher'] = row['PitchersUsed'].split(" (")[0]
    return row

# write a func that removes punctuation from player names using str.replace() and the pattern
def remove_punctuation(text):
    pattern = r'[^\w\s]'
    text = text.replace(pattern, '')
    text = text.replace('ñ', 'n')
    text = text.replace('á', 'a')
    text = text.replace('é', 'e')
    text = text.replace('í', 'i')
    text = text.replace('ó', 'o')
    text = text.replace('ú', 'u')
    text = text.replace("'", "")
    text = text.replace('.', '')
    text = text.replace(' ', '')
    return text

# write a function that can take in the raw games database and return cleaned up and formatted database of games
def format_raw_games(raw_games):
    # remove game num from date
    raw_games['Date'] = raw_games['Date'].apply(remove_game_num)
    # combine date and year column into a single date column
    raw_games['Date'] = raw_games[['Date','Year']].astype(str).apply(', '.join, axis=1)
    # turn date into datetime object
    raw_games['Date'] = pd.to_datetime(raw_games['Date'])
    # assign home and away teams
    raw_games['Home Team'] = raw_games['Team'].where(raw_games['Home'], raw_games['Opp'])
    raw_games['Away Team'] = raw_games['Opp'].where(raw_games['Home'], raw_games['Team'])
    # create home and away score columns
    raw_games = raw_games.apply(home_away_score, axis=1)
    # create starting pitcher columnn for each game in input
    raw_games = raw_games.apply(starting_pitcher, axis=1)
    # remove punctuation from players names
    raw_games['Starting Pitcher'] = raw_games['Starting Pitcher'].apply(remove_punctuation)
    # rename 'Year' column to 'Season'
    raw_games = raw_games.rename(columns={'Year':'Season'})
    # create Game ID column 
    raw_games['Game ID'] = raw_games['Date'].astype(str) + '_' + \
                   raw_games['Home Team'].str.strip() + '_' + \
                   raw_games['Away Team'].str.strip() + '_' + \
                   raw_games['Home Score'].astype(str) + '-' + \
                   raw_games['Away Score'].astype(str)
    # drop uneeded data
    raw_games = raw_games[['Game ID','Date','Season','Home','Team','Home Team','Away Team','Home Score','Away Score','Starting Pitcher']]
    return raw_games

games = format_raw_games(raw_games)

games

# save game pitchers database and starting pitchers.csv
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/games.csv')
games.to_csv(filepath, index=False)

games = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/games.csv')

# write a function that creates target variables based on desired split

# Create a new column that calculates the home team's ouctome margin
games['Outcome'] = games['Home Score'] - games['Away Score']

# Create the target variable categories
# bins = pd.IntervalIndex.from_tuples([(-float('inf'), -5), (-4, -2), (-1,-1), (1,1), (2, 4), (5, float('inf'))])
games['Target Variable'] = pd.cut(games['Outcome'], bins=[-float('inf'), -5, -3, -1, 2, 4, float('inf')], labels=['<=-5', '-3 to -4', '-1 to -2', '1 to 2', '3 to 4', '>=5'])
dummies = pd.get_dummies(games['Target Variable'], prefix='Score')
games = pd.concat([games,dummies], axis=1)

games

# save games w/targets as games_formatted.csv
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/games_formatted.csv')
games.to_csv(filepath, index=False)

# import games_formatted csv
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/games_formatted.csv')
games_f = pd.read_csv(filepath)

games_f

def format_pitch_db(df, period, names=False):
    # if pull% is a column in the database, drop pull%, cent%, and oppo% columns
    if 'Pull%' in df.columns:
        df = df.drop(columns=['Pull%','Cent%', 'Oppo%'])
    # change 'Tm' column to 'Team'
    df = df.rename(columns={'Tm':'Team'})
    # check period
    if period == 'season':
        # set prefix to be '/yr'
        prefix = r'/yr '
        df['Season'] = pd.to_datetime(df['Season'], format='%Y').dt.year
    elif period == 'game':
        # set prefix to be '/g'
        prefix = r'/g '
        df['Date'] = pd.to_datetime(df['Date'])
    if names:
        # modify prefix to by 'sp'+prefix
        prefix = 'sp'+prefix
        # columns to be excluded from prefix being added
        if period == 'season':
            exclude_cols = ['Season','Team','Starting Pitcher']
        elif period == 'game':
            exclude_cols = ['Date','Team','Starting Pitcher']
        # first create a first and last name column
        df[['first name', 'last name']] = df['Name'].str.split(n=1, expand=True)
        # combine first and last name columns to be first initial of first name then last name
        df['Starting Pitcher'] = df['first name'].str[0] + df['last name']
        # remove punctuation from starting pitcher column
        df['Starting Pitcher'] = df['Starting Pitcher'].apply(remove_punctuation)
        # drop columns
        df = df.drop(columns=['Name','IP','TBF','playerId','first name','last name'])
        # Add prefix to all columns except excluded columns
        df = pd.concat([df[exclude_cols], df.drop(columns=exclude_cols).add_prefix(prefix)], axis=1)
    else:
        # modify prefix to by 'rp'+prefix
        prefix = 'rp'+prefix
        # columns to be excluded from prefix being added
        if period == 'season':
            exclude_cols = ['Season','Team']
        elif period == 'game':
            exclude_cols = ['Date','Team']
        df = df.drop(columns=['IP','TBF'])
        # Add prefix to all columns except excluded columns
        df = pd.concat([df[exclude_cols], df.drop(columns=exclude_cols).add_prefix(prefix)], axis=1)
    
    return df

# format starting pitcher advanced stats per year database
sp_adv_y_f = format_pitch_db(sp_adv_y, period='season', names=True)

sp_adv_y_f

# save sp_adv_y as sp_adv_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/sp_adv_y_formatted.csv')
sp_adv_y_f.to_csv(filepath, index=False)

# load sp_adv_y_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/sp_adv_y_formatted.csv')
sp_adv_y_f = pd.read_csv(filepath)

sp_adv_y_f

# format starting pitcher batted ball stats per year database
sp_bat_y_f = format_pitch_db(sp_bat_y, period='season', names=True)

sp_bat_y_f

# save sp_bat_y_f as sp_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/sp_bat_y_formatted.csv')
sp_bat_y_f.to_csv(filepath, index=False)

# load sp_bat_y_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/sp_bat_y_formatted.csv')
sp_bat_y_f = pd.read_csv(filepath)

sp_bat_y_f

# format starting pitcher advanced stats per game database
sp_adv_g_f = format_pitch_db(sp_adv_g, period='game', names=True)

sp_adv_g_f

# save sp_adv_g_f as sp_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/sp_adv_g_formatted.csv')
sp_adv_g_f.to_csv(filepath, index=False)

# load sp_adv_g_f database
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/sp_adv_g_formatted.csv')
sp_adv_g_f = pd.read_csv(filepath)

sp_adv_g_f

# format starting pitcher batted stats per game database
sp_bat_g_f = format_pitch_db(sp_bat_g, period='game', names=True)

sp_bat_g_f

# save sp_bat_g_f as sp_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/sp_bat_g_formatted.csv')
sp_bat_g_f.to_csv(filepath, index=False)

# load sp_bat_g_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/sp_bat_g_formatted.csv')
sp_bat_g_f = pd.read_csv(filepath)

sp_bat_g_f

# format relief pitcher advanced stats per season database
rp_adv_y_f = format_pitch_db(rp_adv_y, period='season')

rp_adv_y_f

# save rp_adv_y_f as rp_adv_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/rp_adv_y_formatted.csv')
rp_adv_y_f.to_csv(filepath, index=False)

# load rp_adv_y_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/rp_adv_y_formatted.csv')
rp_adv_y_f = pd.read_csv(filepath)

rp_adv_y_f

# format relief pitcher batted ball stats per season database
rp_bat_y_f = format_pitch_db(rp_bat_y, period='season')

rp_bat_y_f

# save rp_bat_y as rp_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/rp_bat_y_formatted.csv')
rp_bat_y_f.to_csv(filepath, index=False)

# load rp_bat_y_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/rp_bat_y_formatted.csv')
rp_bat_y_f = pd.read_csv(filepath)

rp_bat_y_f

# format relief pitcher adv stats per game database
rp_adv_g_f = format_pitch_db(rp_adv_g, period='game')

rp_adv_g_f

# save rp_adv_g_f as rp_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/rp_adv_g_formatted.csv')
rp_adv_g_f.to_csv(filepath, index=False)

# load rp_adv_g_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/rp_adv_g_formatted.csv')
rp_adv_g_f = pd.read_csv(filepath)

rp_adv_g_f

# format relief pitcher batted ball stats per game database
rp_bat_g_f = format_pitch_db(rp_bat_g, period='game')

rp_bat_g_f

# save rp_bat_g_f as rp_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/rp_bat_g_formatted.csv')
rp_bat_g_f.to_csv(filepath, index=False)

# load rp_bat_g_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/rp_bat_g_formatted.csv')
rp_bat_g_f = pd.read_csv(filepath)

rp_bat_g_f

def format_batting(df, period,):
    # drop 'PA' column
    df = df.drop(columns=['PA'])
    # if pull% is a column in the database, drop pull%, cent%, and oppo% columns
    if 'Pull%' in df.columns:
        df = df.drop(columns=['Pull%','Cent%', 'Oppo%'])
    # change 'Tm' column to 'Team'
    df = df.rename(columns={'Tm':'Team'})
    # check period
    if period == 'season':
        df['Season'] = pd.to_datetime(df['Season'], format='%Y').dt.year
        exclude_cols = ['Season','Team']
        prefix = r'/yr '
    elif period == 'game':
        df['Date'] = pd.to_datetime(df['Date'])
        exclude_cols = ['Date','Team']
        prefix = r'/g '
    
    df = pd.concat([df[exclude_cols], df.drop(columns=exclude_cols).add_prefix(prefix)], axis=1)
    
    return df

# format team adv batting per year
team_adv_bat_y_f = format_batting(team_adv_bat_y, period='season')

team_adv_bat_y_f

# save team_adv_bat_y_f as team_adv_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_adv_bat_y_formatted.csv')
team_adv_bat_y_f.to_csv(filepath, index=False)

#load team_adv_bat_y_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_adv_bat_y_formatted.csv')
team_adv_bat_y_f = pd.read_csv(filepath)

team_adv_bat_y_f

# format team bat batting per year
team_bat_bat_y_f = format_batting(team_bat_bat_y, period='season')

team_bat_bat_y_f

# save team_bat_bat_y_f as team_adv_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_bat_bat_y_formatted.csv')
team_bat_bat_y_f.to_csv(filepath, index=False)

# load team_bat_bat_y_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_bat_bat_y_formatted.csv')
team_bat_bat_y_f = pd.read_csv(filepath)

team_bat_bat_y_f

# format team adv batting per game
team_adv_bat_g_f = format_batting(team_adv_bat_g, period='game')

team_adv_bat_g_f

# save team_adv_bat_g_f as team_adv_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_adv_bat_g_formatted.csv')
team_adv_bat_g_f.to_csv(filepath, index=False)

# load team_adv_bat_g_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_adv_bat_g_formatted.csv')
team_adv_bat_g_f = pd.read_csv(filepath)

team_adv_bat_g_f

# format team bat batting per game
team_bat_bat_g_f = format_batting(team_bat_bat_g, period='game')

team_bat_bat_g_f

# save team_bat_bat_g_f as team_adv_bat_y_formatted as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_bat_bat_g_formatted.csv')
team_bat_bat_g_f.to_csv(filepath, index=False)

# load team_bat_bat_g_f
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/team_bat_bat_g_formatted.csv')
team_bat_bat_g_f = pd.read_csv(filepath)

team_bat_bat_g_f

games_f

sp_adv_y_f

# merge sp_adv_y_f with the games data on the 'Season', 'team', and 'starting pitcher' column
merged = pd.merge(games_f, sp_adv_y_f, on=['Season', 'Team', 'Starting Pitcher'])

# merge sp_bat_y_f with the games data on the 'Season', 'team', and 'starting pitcher' column
merged = pd.merge(merged, sp_bat_y_f, on=['Season', 'Team', 'Starting Pitcher'])

# merge rp_adv_y_f with the games data on the 'Season', 'team' column
merged = pd.merge(merged, rp_adv_y_f, on=['Season', 'Team'])

# merge rp_bat_y_f with the games data on the 'Season', 'team' column
merged = pd.merge(merged, rp_bat_y_f, on=['Season', 'Team'])

# merge team_adv_bat_y_f with the games data on the 'Season', 'team' column
merged = pd.merge(merged, team_adv_bat_y_f, on=['Season', 'Team'])

# merge team_bat_bat_y_f with the games data on the 'Season', 'team' column
merged = pd.merge(merged, team_bat_bat_y_f, on=['Season', 'Team'])

# merge team_def_data_f with the games data on the 'Season', 'team' column
merged = pd.merge(merged, team_def_data_f, on=['Season', 'Team'])

# save merged as csv file
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/merged.csv')
merged.to_csv(filepath, index=False)

# load merged
filepath = Path('/content/drive/MyDrive/Colab Notebooks/Projects/NeuralNet/blogs/Multinomial Logistic Regression/data files/fangraphs data/merged.csv')
merged = pd.read_csv(filepath)

merged

for col in merged.columns:
    print(col)

# write a function that can take in a dataframe of stats per game and spit out 
# a dataframe of average stats over last 'x' amount of days

# cols = []
def get_rolling_avg(df, days, names=False):

    # convert days param to string and add 'D' character to represent calendar days when passed into rolling function
    days = str(days)+'D'
    # make sure date column in of datetime type
    df['Date'] = pd.to_datetime(df['Date'])
    if names:
        rolling_avg = df.groupby(['Team', 'Starting Pitcher']).rolling('10D', on='Date', min_periods=1).mean()
    else:
        rolling_avg = df.groupby('Team').rolling('10D', on='Date', min_periods=1).mean()
    # add 'x' day avg to column name
    rolling_avg = rolling_avg.add_prefix(f'{days} day ')
    rolling_avg = rolling_avg.reset_index()
    if names:
        rolling_avg = rolling_avg.drop(columns='level_2')
    else:
        rolling_avg = rolling_avg.drop(columns='level_1')
    return rolling_avg

# set days variable to calculate rolling average of
days = 10
sp_days = 15

# get rolling averages for for all categories using per game databases
sp_adv_g_avg = get_rolling_avg(sp_adv_g_f, sp_days, names=True)
sp_bat_g_avg = get_rolling_avg(sp_bat_g_f, sp_days, names=True)
rp_adv_g_avg = get_rolling_avg(rp_adv_g_f, days)
rp_bat_g_avg = get_rolling_avg(rp_bat_g_f, days)
team_adv_bat_g_avg = get_rolling_avg(team_adv_bat_g_f, days)
team_bat_bat_g_avg = get_rolling_avg(team_bat_bat_g_f, days)

def Regressor(X, Y, iterations, learning_rate, use_GPU=False):
    
    if use_GPU:
        X = cp.array(X)
        Y = cp.array(Y)
        Theta = cp.random.randn(X.shape[1], 1)
    else:
        X = np.array(X)
        Y = np.array(Y)
        Theta = np.random.randn(X.shape[1], 1)

    mses = []
    for i in range(iterations):
        if use_GPU:
            Yh = X.dot(Theta)
            residual = Y - Yh
            mse = cp.sum((cp.square(residual))) / len(Y)
            mses.append(mse)
            Theta -= (learning_rate / len(Yh)) * (-2 * X.T.dot(residual))
        else:
            Yh = X.dot(Theta)
            residual = Y - Yh
            mse = np.sum((np.square(residual))) / len(Y)
            mses.append(mse)
            Theta -= (learning_rate / len(Yh)) * (-2 * X.T.dot(residual))

    return mses, Theta

def r_squared(Y,predictions):
    # calculate residual sum of squares
    residuals = Y-predictions
    SSR = np.sum(residuals**2)
    # calculate total sum of squares
    SST=np.sum(np.square(Y-Y.mean()))
    # calculate coefficient of determination
    r_sq = 1 - (SSR / SST)
    
    return r_sq

def VIF(r_sq):
    return 1/(1-r_sq)

class StandardScaler():
    def __init__(self, use_GPU=False):
        self.mean = None
        self.standard_deviation = None
        self.use_GPU = use_GPU

    def fit(self, data):
        if self.use_GPU:
            data = cp.array(data)
            self.mean = cp.mean(data, axis=0)
            self.standard_deviation = cp.sqrt(cp.mean(cp.square(data - self.mean), axis=0))
        else:
            self.mean = np.mean(data, axis=0)
            self.standard_deviation = np.sqrt(np.mean(np.square(data - self.mean), axis=0))

    def transform(self, data):
        if self.use_GPU:
            data = cp.array(data)
            return (data - self.mean) / self.standard_deviation
        else:
            return (data - self.mean) / self.standard_deviation

def collinearity_test(data, iterations, learning_rate, use_GPU=False):
    r2s = []
    VIFs = []

    for col in data.columns:
        X = data.drop(columns=col).to_numpy()
        Y = data[col].to_numpy().reshape((-1, 1))

        scaler = StandardScaler(use_GPU)
        scaler.fit(X)
        X = scaler.transform(X)

        if use_GPU:
            X = cp.column_stack((cp.ones((X.shape[0], 1)), X))
            _, Theta = Regressor(X, Y, iterations, learning_rate, use_GPU)
            Theta = cp.asnumpy(Theta)
            Y = cp.asnumpy(Y)
            X = cp.asnumpy(X)
        else:
            X = np.column_stack((np.ones((X.shape[0], 1)), X))
            _, Theta = Regressor(X, Y, iterations, learning_rate, use_GPU)

        # calculate r-squared
        r_sq = r_squared(Y,X.dot(Theta))
        r2s.append(r_sq)
        # calculate VIF
        vif = VIF(r_sq)
        VIFs.append(vif)

    return r2s, VIFs

# write a function that takes in a pandas dataframe, adds a column for the bias term, and returns an input and target dataframe
def format_input(df, collinearity_test=False):
    if collinearity_test:
        input = df.drop(columns=['Game ID','Date','Season','Home','Team','Home Team','Away Team','Home Score','Away Score','Starting Pitcher','Target Variable','Score_<=-5','Score_-3 to -4','Score_-1 to -2','Score_1 to 2','Score_3 to 4','Score_>=5'])
        return input
    else:
        df = df.drop(columns=['Game ID','Date','Season','Home','Team','Home Team','Away Team','Home Score','Away Score','Starting Pitcher','Outcome','Target Variable'])
        input = df.drop(columns=['Score_<=-5','Score_-3 to -4','Score_-1 to -2','Score_1 to 2','Score_3 to 4','Score_>=5'])
        target = df[['Score_<=-5','Score_-3 to -4','Score_-1 to -2','Score_1 to 2','Score_3 to 4','Score_>=5']]
        return input, target

# pass in dataframe of input data and get a numpy array with bias term added
input = format_input(merged, True)

input

# run a collinearity test on input features
r2s, VIFs = collinearity_test(input, 1000, 0.01, use_GPU=True)

VIFs

collin = pd.DataFrame()
collin['Feature Variable'] = input.columns
collin['r2s'] = r2s
collin['VIFs'] = VIFs

collin

# remove RngR and UZR due to extremely high VIF's and rerun collinearity test
input = input.drop(columns=['RngR','UZR'])
r2s, VIFs = collinearity_test(input, 1000, 0.01)
collin = pd.DataFrame()
collin['Feature Variable'] = input.columns
collin['r2s'] = r2s
collin['VIFs'] = VIFs
collin

## code k-fold cross validation ##
def cross_validate(X, Y, iterations, learning_rate):

    #scale data
    scaler = StandardScaler()
    scaler.fit(X)
    X = scaler.transform(X)

    # add column of ones to X
    X = np.column_stack((np.ones((X.shape[0],1)),X))

    # initialize theta term
    theta = np.random.randn(X.shape[1],1)

    # split data into train and validation set
    ## shuffle data
    p = np.random.permutation(len(X))
    X = X[p]
    Y = Y[p]

    ## split
    test_percent = 10
    test_size = len(X) // test_percent

    x_train = X[:-test_size]
    y_train = Y[:-test_size]
    x_test = X[-test_size:]
    y_test = Y[-test_size:]

    # split train into k folds
    folds = 10
    fold_size = len(x_train) // folds

    # initialize array to store mses
    mses = []

    for i in range(iterations):
        # reset fold mses array to be empty
        fold_mses = []

        for k in range(folds):
            # get range of indices to use as test fold
            test_fold = np.arange(k*fold_size,(k+1)*fold_size)
            # set train and test data according to current test fold
            x_train_fold = np.delete(x_train, test_fold, axis=0)
            y_train_fold = np.delete(y_train, test_fold, axis=0)
            x_test_fold = np.take(x_train, test_fold, axis=0)
            y_test_fold = np.take(y_train, test_fold, axis=0)

            # run regression on training folds
            Yh = x_train_fold.dot(theta)
            residual = y_train_fold - Yh
            train_mse = np.sum((np.square(residual)))/len(y_train_fold)
            theta -= (learning_rate/len(Yh))*(-2*x_train_fold.T.dot(residual))

            # test on test fold
            Yh = x_test_fold.dot(theta)
            residual = y_test_fold - Yh
            test_mse = np.sum((np.square(residual)))/len(y_test_fold)

            fold_mses.append([train_mse, test_mse])

        mses.append(np.mean(fold_mses, axis=0))
        # if i % 10 == 0:
        #     print(f'Iteration {i}: {mses[-1]}')

    # test validation data to determine true MSE
    Yh = x_test.dot(theta)
    residual = y_test - Yh
    mse = np.sum((np.square(residual)))/len(y_test)
    # print(f'Final MSE on validation data: {mse}')

    return mse, mses, {'x_train':x_train, 'y_train':y_train, 'x_test':x_test, 'y_test':y_test, 'theta':theta}

# The r-squared of the outcome variable is lower....and VIFs are still all over the place.  I am going to
# try a simple regression model to see how well I can predict the outcome variable to get a baseline



scaler = StandardScaler(use_GPU=True)